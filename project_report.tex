\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}

\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Sentiment Analysis Project Report}
\fancyhead[R]{Dhruv Mishra - 2210110674}
\fancyfoot[C]{\thepage}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Sentiment Analysis Project Report},
    pdfauthor={Dhruv Mishra}
}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=2
}

\title{\textbf{Comprehensive Sentiment Analysis System: A Comparative Study of Multiple Algorithms}\\[0.5cm]
\large Project Report}
\author{Dhruv Mishra\\Roll No: 2210110674}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This project presents a comprehensive sentiment analysis system that implements and compares multiple algorithms ranging from traditional machine learning approaches to state-of-the-art transformer-based models. The system evaluates eleven different algorithms including Naive Bayes, DistilBERT, RoBERTa, ALBERT, DeBERTa, ELECTRA, TF-IDF with Cosine Similarity, Support Vector Machine (SVM), Logistic Regression, VADER, and TextBlob. The evaluation is performed on a carefully curated dataset of 150 movie reviews covering positive, negative, neutral, and complex cases including double negations and sarcasm. Performance metrics including accuracy, precision, recall, F1-score, and average confidence are calculated and compared. The results demonstrate that transformer-based models, particularly RoBERTa and DistilBERT, achieve superior performance with accuracy rates of 78.67\% and 76.67\% respectively, while traditional methods provide faster inference at the cost of reduced accuracy. The system is implemented with three user interfaces: a command-line interface, a web application, and a graphical user interface, providing comprehensive visualization and comparison capabilities.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Background}
Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) task that involves determining the emotional tone or attitude expressed in a piece of text. With the exponential growth of user-generated content on social media platforms, review websites, and online forums, sentiment analysis has become crucial for businesses, researchers, and organizations to understand public opinion, customer satisfaction, and market trends.

The challenge of sentiment analysis lies in understanding the nuanced ways humans express emotions, including context-dependent meanings, sarcasm, double negations, and cultural variations. Traditional rule-based and statistical methods have been supplemented and, in many cases, superseded by deep learning approaches, particularly transformer-based models that can capture complex linguistic patterns.

\subsection{Problem Statement}
The primary objective of this project is to develop a comprehensive sentiment analysis system that:
\begin{itemize}
    \item Implements multiple sentiment analysis algorithms spanning different paradigms
    \item Provides a fair and comprehensive comparison of algorithm performance
    \item Handles edge cases including double negations, sarcasm, and neutral sentiments
    \item Offers multiple user interfaces for different use cases
    \item Visualizes performance metrics and provides actionable insights
\end{itemize}

\subsection{Objectives}
\begin{enumerate}
    \item Implement eleven different sentiment analysis algorithms
    \item Create a comprehensive test dataset with edge cases
    \item Evaluate and compare algorithm performance using standard metrics
    \item Develop multiple user interfaces (CLI, Web, GUI)
    \item Generate visualizations for performance analysis
    \item Provide AI-powered recommendations for algorithm selection
\end{enumerate}

\subsection{Scope}
This project focuses on binary sentiment classification (Positive/Negative) for English text, specifically movie reviews. The system is designed to be extensible and can be adapted for other domains and languages with appropriate modifications.

\section{Literature Review}

\subsection{Traditional Machine Learning Approaches}
Traditional machine learning methods for sentiment analysis have been widely studied. \textbf{Naive Bayes} classifiers, based on Bayes' theorem with independence assumptions, have been popular due to their simplicity and effectiveness \cite{mccallum1998comparison}. \textbf{Support Vector Machines (SVM)} have shown strong performance in text classification tasks by finding optimal decision boundaries \cite{cortes1995support}. \textbf{Logistic Regression} provides interpretable models with probabilistic outputs, making it valuable for understanding feature importance \cite{cox1958regression}.

\subsection{Feature Extraction Methods}
\textbf{TF-IDF (Term Frequency-Inverse Document Frequency)} has been a cornerstone of text feature extraction, weighting terms based on their importance in documents \cite{salton1988term}. When combined with \textbf{Cosine Similarity}, it provides a simple yet effective method for comparing text similarity and sentiment.

\subsection{Transformer-Based Models}
The introduction of transformer architectures revolutionized NLP. \textbf{BERT (Bidirectional Encoder Representations from Transformers)} introduced bidirectional context understanding \cite{devlin2018bert}. \textbf{DistilBERT} achieved 60\% size reduction while maintaining 97\% of BERT's performance \cite{sanh2019distilbert}. \textbf{RoBERTa} improved upon BERT through optimized training procedures \cite{liu2019roberta}. \textbf{ALBERT} reduced model size through parameter sharing and factorization \cite{lan2019albert}. \textbf{DeBERTa} introduced disentangled attention mechanisms \cite{he2020deberta}, while \textbf{ELECTRA} used a more efficient pre-training approach \cite{clark2020electra}.

\subsection{Rule-Based and Lexicon-Based Methods}
\textbf{VADER (Valence Aware Dictionary and sEntiment Reasoner)} is specifically designed for social media text, incorporating rules for capitalization, punctuation, and intensifiers \cite{hutto2014vader}. \textbf{TextBlob} provides a simple lexicon-based approach with pattern analysis capabilities.

\section{Methodology}

\subsection{System Architecture}
The system is built using a modular architecture with the following components:
\begin{itemize}
    \item \textbf{Sentiment Analyzer Module}: Core module implementing all algorithms
    \item \textbf{Algorithm Comparator}: Evaluates and compares algorithm performance
    \item \textbf{User Interfaces}: CLI, Web (Flask), and GUI (CustomTkinter)
    \item \textbf{Visualization Module}: Generates performance charts and graphs
\end{itemize}

\subsection{Algorithms Implemented}

\subsubsection{1. Naive Bayes (Multinomial)}
Naive Bayes is a probabilistic classifier based on Bayes' theorem with the "naive" assumption of feature independence. For sentiment analysis:

\begin{equation}
P(Sentiment|words) = \frac{P(words|Sentiment) \times P(Sentiment)}{P(words)}
\end{equation}

The algorithm:
\begin{enumerate}
    \item Converts text to TF-IDF vectors
    \item Calculates word probabilities for each sentiment class from training data
    \item Computes posterior probabilities using Bayes' theorem
    \item Selects the class with highest probability
\end{enumerate}

\textbf{Advantages}: Fast, simple, works well with small datasets, interpretable.\\
\textbf{Disadvantages}: Assumes word independence, struggles with context and negations.

\subsubsection{2. DistilBERT}
DistilBERT is a distilled version of BERT, 60\% smaller and 60\% faster while maintaining 97\% of BERT's performance. It uses:
\begin{itemize}
    \item 6 transformer layers (vs BERT's 12)
    \item Multi-head self-attention mechanism
    \item Knowledge distillation from BERT
    \item Fine-tuned on SST-2 dataset
\end{itemize}

The model processes text through:
\begin{enumerate}
    \item WordPiece tokenization
    \item Embedding layer (token + position + segment)
    \item 6 transformer layers with self-attention
    \item Classification head for sentiment prediction
\end{enumerate}

\subsubsection{3. RoBERTa}
RoBERTa (Robustly Optimized BERT) improves upon BERT through:
\begin{itemize}
    \item Longer training with more data
    \item Dynamic masking (vs static in BERT)
    \item Removal of next-sentence prediction task
    \item Larger batch sizes
    \item Byte-level BPE tokenization
\end{itemize}

The project uses a Twitter-specialized variant (\texttt{cardiffnlp/twitter-roberta-base-sentiment-latest}), making it particularly effective for social media and informal text.

\subsubsection{4. ELECTRA}
ELECTRA uses a generator-discriminator approach:
\begin{itemize}
    \item \textbf{Generator}: Replaces tokens with plausible alternatives
    \item \textbf{Discriminator}: Identifies which tokens were replaced
    \item More sample-efficient than masked language modeling
    \item Trains on all tokens, not just masked ones
\end{itemize}

This approach provides better pre-training efficiency and often superior downstream performance.

\subsubsection{5. TF-IDF + Cosine Similarity}
This method combines:
\begin{itemize}
    \item \textbf{TF-IDF Vectorization}: 
    \begin{equation}
    TF\text{-}IDF(t,d) = TF(t,d) \times \log\left(\frac{N}{DF(t)}\right)
    \end{equation}
    where $N$ is total documents and $DF(t)$ is document frequency of term $t$.
    
    \item \textbf{Cosine Similarity}:
    \begin{equation}
    \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \times ||\mathbf{B}||}
    \end{equation}
\end{itemize}

The algorithm compares input text against positive and negative reference sets, assigning sentiment based on higher similarity.

\subsubsection{6. Support Vector Machine (SVM)}
SVM finds the optimal hyperplane separating positive and negative classes by maximizing the margin. For text classification:
\begin{itemize}
    \item Maps text features to high-dimensional space
    \item Finds maximum-margin hyperplane
    \item Uses linear kernel for efficiency
    \item Classifies based on which side of hyperplane text falls
\end{itemize}

The optimization problem:
\begin{equation}
\min_{\mathbf{w},b} \frac{1}{2}||\mathbf{w}||^2 + C\sum_{i=1}^{n}\xi_i
\end{equation}
subject to $y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i$.

\subsubsection{7. Logistic Regression}
Logistic Regression models probability using the sigmoid function:
\begin{equation}
P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w} \cdot \mathbf{x} + b)}}
\end{equation}

The algorithm:
\begin{enumerate}
    \item Learns feature weights during training
    \item Computes weighted sum of features
    \item Applies sigmoid function for probability
    \item Classifies as positive if $P > 0.5$
\end{enumerate}

\subsection{Test Dataset}
A comprehensive test dataset of 150 movie reviews was created, including:
\begin{itemize}
    \item 50 positive reviews
    \item 50 negative reviews
    \item 35 neutral reviews
    \item 15 complex cases (double negations, sarcasm, edge cases)
\end{itemize}

Examples of edge cases:
\begin{itemize}
    \item Double negations: "Not a bad movie", "Not terrible"
    \item Sarcasm: "Oh great, another masterpiece"
    \item Context-dependent: "This movie is so bad it's good"
\end{itemize}

\section{Implementation}

\subsection{Technology Stack}
\begin{itemize}
    \item \textbf{Programming Language}: Python 3.8+
    \item \textbf{ML Libraries}: scikit-learn, transformers, torch
    \item \textbf{NLP Libraries}: vaderSentiment, textblob
    \item \textbf{Web Framework}: Flask
    \item \textbf{GUI Framework}: CustomTkinter
    \item \textbf{Visualization}: Matplotlib
    \item \textbf{Data Processing}: NumPy, Pandas
\end{itemize}

\subsection{System Components}

\subsubsection{Core Modules}
\begin{itemize}
    \item \texttt{src/sentiment\_analyzer.py}: Implements all sentiment analysis algorithms
    \item \texttt{src/algorithm\_comparison.py}: Evaluates and compares algorithms
    \item \texttt{src/test\_reviews.py}: Contains test dataset
\end{itemize}

\subsubsection{User Interfaces}
\begin{itemize}
    \item \texttt{cli.py}: Command-line interface
    \item \texttt{app.py}: Flask web application
    \item \texttt{gui.py}: CustomTkinter graphical interface
\end{itemize}

\subsection{Performance Evaluation}
The evaluation process:
\begin{enumerate}
    \item Loads test dataset (150 reviews with labels)
    \item Runs each algorithm on all test samples
    \item Calculates metrics: Accuracy, Precision, Recall, F1-score
    \item Computes average confidence scores
    \item Generates confusion matrices
    \item Creates visualizations
\end{enumerate}

\section{Results and Performance Analysis}

\subsection{Performance Metrics}
The following metrics were calculated for each algorithm:

\begin{itemize}
    \item \textbf{Accuracy}: Percentage of correct predictions
    \item \textbf{Precision}: True Positives / (True Positives + False Positives)
    \item \textbf{Recall}: True Positives / (True Positives + False Negatives)
    \item \textbf{F1-Score}: Harmonic mean of precision and recall
    \item \textbf{Average Confidence}: Mean confidence score across all predictions
\end{itemize}

\subsection{Performance Table}

\begin{table}[H]
\centering
\caption{Comprehensive Performance Comparison of Sentiment Analysis Algorithms}
\label{tab:performance}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Avg Confidence} & \textbf{Type} \\
 & (\%) & (\%) & (\%) & (\%) & (\%) & \\
\midrule
\textbf{RoBERTa} & \textbf{78.67} & \textbf{93.33} & 66.67 & 77.78 & 85.85 & Transformer \\
\textbf{DistilBERT} & \textbf{76.67} & 83.56 & 72.62 & \textbf{77.71} & \textbf{98.58} & Transformer \\
ELECTRA & 68.67 & 74.67 & 66.67 & 70.44 & 78.91 & Transformer \\
Naive Bayes & 63.33 & 67.90 & 65.48 & 66.67 & 70.98 & Traditional ML \\
SVM & 61.33 & 66.67 & 61.90 & 64.20 & 67.16 & Traditional ML \\
Logistic Regression & 55.33 & 68.09 & 38.10 & 48.85 & 52.92 & Traditional ML \\
TF-IDF + Cosine & 54.67 & 75.00 & 28.57 & 41.38 & 45.59 & Similarity-based \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Detailed Analysis}

\subsubsection{Transformer-Based Models}
Transformer models demonstrate superior performance:
\begin{itemize}
    \item \textbf{RoBERTa} achieves the highest accuracy (78.67\%) and precision (93.33\%), making it ideal when false positives must be minimized
    \item \textbf{DistilBERT} provides the best balance with 76.67\% accuracy and highest F1-score (77.71\%), with exceptional confidence scores (98.58\%)
    \item \textbf{ELECTRA} shows moderate performance (68.67\% accuracy) but may benefit from better fine-tuning
\end{itemize}

\subsubsection{Traditional Machine Learning}
Traditional ML methods offer faster inference:
\begin{itemize}
    \item \textbf{Naive Bayes} performs best among traditional methods (63.33\% accuracy)
    \item \textbf{SVM} shows comparable performance (61.33\% accuracy) with good generalization
    \item \textbf{Logistic Regression} struggles with this dataset (55.33\% accuracy), possibly due to limited training data
\end{itemize}

\subsubsection{Similarity-Based Methods}
\begin{itemize}
    \item \textbf{TF-IDF + Cosine Similarity} shows lowest accuracy (54.67\%) but highest precision among non-transformers (75.00\%), indicating conservative predictions
\end{itemize}

\subsection{Confusion Matrix Analysis}

\begin{table}[H]
\centering
\caption{Confusion Matrix Details (True Positive, False Positive, False Negative, True Negative)}
\label{tab:confusion}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Algorithm} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} \\
\midrule
RoBERTa & 56 & 4 & 28 & 62 \\
DistilBERT & 61 & 12 & 23 & 54 \\
ELECTRA & 56 & 19 & 28 & 47 \\
Naive Bayes & 55 & 26 & 29 & 40 \\
SVM & 52 & 26 & 32 & 40 \\
Logistic Regression & 32 & 15 & 52 & 51 \\
TF-IDF + Cosine & 24 & 8 & 60 & 58 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Observations}

\begin{enumerate}
    \item \textbf{Precision vs Recall Trade-off}: RoBERTa shows high precision (93.33\%) but lower recall (66.67\%), while DistilBERT achieves better balance
    \item \textbf{Confidence Scores}: DistilBERT shows exceptional confidence (98.58\%), indicating high model certainty
    \item \textbf{Edge Case Handling}: Transformer models generally handle complex cases better than traditional methods
    \item \textbf{Computational Efficiency}: Traditional methods (Naive Bayes, SVM) provide faster inference suitable for real-time applications
\end{enumerate}

\section{Discussion}

\subsection{Algorithm Selection Guidelines}

Based on the performance analysis, algorithm selection should consider:

\begin{enumerate}
    \item \textbf{Maximum Accuracy}: RoBERTa (78.67\%) - Best overall performance
    \item \textbf{Balanced Performance}: DistilBERT (F1: 77.71\%) - Best trade-off between precision and recall
    \item \textbf{Minimize False Positives}: RoBERTa (Precision: 93.33\%) - Critical for applications like spam detection
    \item \textbf{Minimize False Negatives}: DistilBERT (Recall: 72.62\%) - Important for applications like medical diagnosis
    \item \textbf{High Confidence}: DistilBERT (98.58\%) - Most reliable for production systems
    \item \textbf{Speed Requirements}: Naive Bayes or SVM - Fast inference for real-time applications
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item Test dataset size (150 samples) may not fully represent all edge cases
    \item Some algorithms (ALBERT, DeBERTa, VADER, TextBlob) were not evaluated due to initialization issues
    \item Binary classification may not capture nuanced sentiments
    \item Domain-specific: Trained/tested on movie reviews, may not generalize to other domains
    \item Computational resources: Transformer models require significant memory and processing power
\end{itemize}

\subsection{Future Work}

\begin{enumerate}
    \item Expand test dataset to include more diverse domains
    \item Implement multi-class sentiment classification (Positive, Negative, Neutral)
    \item Fine-tune transformer models on domain-specific data
    \item Implement ensemble methods combining multiple algorithms
    \item Add support for other languages
    \item Optimize inference speed for production deployment
    \item Implement active learning for continuous improvement
\end{enumerate}

\section{Conclusion}

This project successfully implements and compares multiple sentiment analysis algorithms, demonstrating the evolution from traditional machine learning approaches to state-of-the-art transformer models. The comprehensive evaluation on a carefully curated dataset reveals that transformer-based models, particularly RoBERTa and DistilBERT, achieve superior performance with accuracy rates exceeding 76\%. However, traditional methods like Naive Bayes and SVM remain valuable for applications requiring fast inference or limited computational resources.

The system's multi-interface design (CLI, Web, GUI) makes it accessible for various use cases, from research to production deployment. The visualization and comparison capabilities provide valuable insights for algorithm selection based on specific requirements.

The results highlight the importance of considering multiple factors beyond raw accuracy, including precision, recall, confidence scores, and computational efficiency. The AI-powered recommendations feature assists users in selecting the most appropriate algorithm for their specific needs.

This work contributes to the understanding of sentiment analysis algorithm performance and provides a practical framework for comparative evaluation in real-world applications.

\section{Acknowledgments}

The author acknowledges the open-source community for providing excellent libraries and pre-trained models that made this project possible, including Hugging Face Transformers, scikit-learn, and the research teams behind the various algorithms implemented.

\newpage
\section{References}

\begin{thebibliography}{99}

\bibitem{mccallum1998comparison}
McCallum, A., \& Nigam, K. (1998). A comparison of event models for naive bayes text classification. \textit{AAAI-98 workshop on learning for text categorization}, 752(1), 41-48.

\bibitem{cortes1995support}
Cortes, C., \& Vapnik, V. (1995). Support-vector networks. \textit{Machine learning}, 20(3), 273-297.

\bibitem{cox1958regression}
Cox, D. R. (1958). The regression analysis of binary sequences. \textit{Journal of the Royal Statistical Society: Series B (Methodological)}, 20(2), 215-232.

\bibitem{salton1988term}
Salton, G., \& Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. \textit{Information processing \& management}, 24(5), 513-523.

\bibitem{devlin2018bert}
Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. \textit{arXiv preprint arXiv:1810.04805}.

\bibitem{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., \& Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. \textit{arXiv preprint arXiv:1910.01108}.

\bibitem{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... \& Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. \textit{arXiv preprint arXiv:1907.11692}.

\bibitem{lan2019albert}
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., \& Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. \textit{arXiv preprint arXiv:1909.11942}.

\bibitem{he2020deberta}
He, P., Liu, X., Gao, J., \& Chen, W. (2020). Deberta: Decoding-enhanced bert with disentangled attention. \textit{arXiv preprint arXiv:2006.03654}.

\bibitem{clark2020electra}
Clark, K., Luong, M. T., Le, Q. V., \& Manning, C. D. (2020). Electra: Pre-training text encoders as discriminators rather than generators. \textit{arXiv preprint arXiv:2003.10555}.

\bibitem{hutto2014vader}
Hutto, C., \& Gilbert, E. (2014). Vader: A parsimonious rule-based model for sentiment analysis of social media text. \textit{Proceedings of the international AAAI conference on web and social media}, 8(1), 216-225.

\end{thebibliography}

\newpage
\appendix
\section{Appendix A: System Requirements}

\subsection{Software Requirements}
\begin{itemize}
    \item Python 3.8 or higher
    \item pip package manager
    \item Virtual environment (recommended)
\end{itemize}

\subsection{Python Dependencies}
\begin{verbatim}
transformers>=4.20.0
torch>=1.12.0
scikit-learn>=1.0.0
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.5.0
flask>=2.0.0
customtkinter>=5.0.0
vaderSentiment>=3.3.2
textblob>=0.17.1
Pillow>=9.0.0
\end{verbatim}

\section{Appendix B: Installation Instructions}

\begin{enumerate}
    \item Clone the repository
    \item Create virtual environment: \texttt{python -m venv venv}
    \item Activate virtual environment: \texttt{source venv/bin/activate} (Linux/Mac) or \texttt{venv\textbackslash Scripts\textbackslash activate} (Windows)
    \item Install dependencies: \texttt{pip install -r requirements.txt}
    \item Run CLI: \texttt{python cli.py}
    \item Run Web App: \texttt{python app.py}
    \item Run GUI: \texttt{python gui.py}
\end{enumerate}

\section{Appendix C: Code Structure}

\begin{verbatim}
SentimentAwareMovieRecommender/
├── src/
│   ├── sentiment_analyzer.py      # Core sentiment analysis module
│   ├── algorithm_comparison.py     # Performance evaluation module
│   └── test_reviews.py             # Test dataset
├── models/                         # Pre-trained models
├── cli.py                          # Command-line interface
├── app.py                          # Flask web application
├── gui.py                          # Graphical user interface
├── templates/                      # HTML templates
├── requirements.txt                # Python dependencies
└── README.md                       # Project documentation
\end{verbatim}

\end{document}

